{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "\n",
    "if \"/usr/src\" not in sys.path:\n",
    "    sys.path.append(\"/usr/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "\n",
    "import helper_plots as hplt\n",
    "import data.dataset, data.transform\n",
    "\n",
    "# from model.vae import Encoder, Decoder\n",
    "from models.helper_train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 111\n",
    "LEARNING_RATE = 0.0005\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "LATENT_SIZE = 4\n",
    "RECONSTRUCTION_TERM_WEIGHT = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample's size = 1528\n",
      "Test sample's size = 382\n"
     ]
    }
   ],
   "source": [
    "# Directory with source data\n",
    "\n",
    "cells_dir = \"/usr/src/data/separated_bacterias\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        data.transform.CellPadResize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = data.dataset.CellsImageDataset(cells_dir, transform=transform)\n",
    "\n",
    "test_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "print(\"Train sample's size =\", train_size)\n",
    "print(\"Test sample's size =\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Image batch dimensions: torch.Size([128, 1, 64, 64])\n",
      "\n",
      "Testing Set:\n",
      "Image batch dimensions: torch.Size([128, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset\n",
    "print(\"Training Set:\")\n",
    "for images in train_loader:\n",
    "    print(\"Image batch dimensions:\", images.size())\n",
    "    break\n",
    "\n",
    "# Checking the dataset\n",
    "print(\"\\nTesting Set:\")\n",
    "for images in test_loader:\n",
    "    print(\"Image batch dimensions:\", images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trim(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.size = image_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, : self.size, : self.size]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.convolution_seria = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "            nn.Flatten(),  # (N, 64, 8, 8) -> (N, 4096)\n",
    "        )\n",
    "\n",
    "        self.z_mean = torch.nn.Linear(4096, self.latent_size)\n",
    "        self.z_log_var = torch.nn.Linear(4096, self.latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolution_seria(x)\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_log_var = self.z_log_var(x)\n",
    "        eps = torch.randn((z_mean.size(0), self.latent_size)).to(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        z = z_mean + eps * torch.exp(z_log_var / 2.0)\n",
    "        return z, z_mean, z_log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.convolution_transpose_seria = nn.Sequential(\n",
    "            torch.nn.Linear(self.latent_size, 4096),\n",
    "            nn.Unflatten(1, (64, 8, 8)),\n",
    "            nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=0),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0),\n",
    "            Trim(self.image_size),  # 1x65x65 -> 1x64x64\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convolution_transpose_seria(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.latent_size = encoder.latent_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x = batch\n",
    "\n",
    "        z, z_mean, z_log_var = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "\n",
    "        pixelwise = torch.nn.functional.mse_loss(x_hat, x, reduction=\"none\")\n",
    "        pixelwise = pixelwise.view(batch.size(0), -1).sum(dim=1)\n",
    "        pixelwise = pixelwise.mean()\n",
    "\n",
    "        kl_div = -0.5 * torch.sum(\n",
    "            1 + z_log_var - z_mean**2 - torch.exp(z_log_var), axis=1\n",
    "        )  # sum over latent dimension  # type: ignore\n",
    "        kl_div = kl_div.mean()  # average over batch dimension\n",
    "\n",
    "        loss = pixelwise + kl_div\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_combined_loss\", loss)\n",
    "        self.log(\"pixelwise_loss\", pixelwise)\n",
    "        self.log(\"kl_div\", kl_div)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(IMAGE_SIZE, LATENT_SIZE)\n",
    "decoder = Decoder(IMAGE_SIZE, LATENT_SIZE)\n",
    "\n",
    "vae = VariationalAutoEncoder(encoder, decoder, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 162 K \n",
      "1 | decoder | Decoder | 150 K \n",
      "------------------------------------\n",
      "312 K     Trainable params\n",
      "0         Non-trainable params\n",
      "312 K     Total params\n",
      "1.250     Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 162 K \n",
      "1 | decoder | Decoder | 150 K \n",
      "------------------------------------\n",
      "312 K     Trainable params\n",
      "0         Non-trainable params\n",
      "312 K     Total params\n",
      "1.250     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1f8b45fe4b4e1ebfc38bcbc52b6a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=100, log_every_n_steps=4)\n",
    "trainer.fit(vae, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot(model, n_images, device, image_name=\"\"):\n",
    "    latent_size = model.latent_size\n",
    "    with torch.no_grad():\n",
    "        rand_features = torch.randn(n_images, latent_size).to(device)\n",
    "        generated_images = model.decoder(rand_features)\n",
    "\n",
    "    grid = make_grid(generated_images, nrow=int(np.sqrt(n_images)))\n",
    "    grid = grid.detach().to(torch.device(\"cpu\"))\n",
    "    grid = np.transpose(grid.numpy(), (1, 2, 0))\n",
    "    plt.imshow(grid, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if image_name:\n",
    "        plt.imsave(image_name, grid, cmap=\"gray\")\n",
    "\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "\n",
    "def out_convolution_shape(h_in, w_in, kernel_size, stride=(1, 1), padding=(0, 0)):\n",
    "    h_out = floor((h_in + 2 * padding[0] - kernel_size[0]) / stride[0] + 1)\n",
    "    w_out = floor((w_in + 2 * padding[1] - kernel_size[1]) / stride[1] + 1)\n",
    "\n",
    "    return h_out, w_out\n",
    "\n",
    "\n",
    "def out_convtranspose_shape(h_in, w_in, kernel_size, stride=(1, 1), padding=(0, 0)):\n",
    "    h_out = (h_in - 1) * stride[0] - 2 * padding[0] + (kernel_size[0] - 1) + 1\n",
    "    w_out = (w_in - 1) * stride[1] - 2 * padding[1] + (kernel_size[1] - 1) + 1\n",
    "\n",
    "    return h_out, w_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec5511c57591959bfe383097d4c77eeb724c9a6ac951d9eef9284c9544b2033b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
