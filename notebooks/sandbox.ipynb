{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "from PIL import Image\n",
    "import os, shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "\n",
    "if \"/usr/src\" not in sys.path:\n",
    "    sys.path.append(\"/usr/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title='', figsize=(3, 3), save_fig=False):\n",
    "    \"\"\" Show image from FloatTensor\n",
    "\n",
    "    Params:\n",
    "    image - The source tensor\n",
    "    title - The string for title of the figure\n",
    "    figsize - The size in dpi\n",
    "    save_fig - Is it need to save picture\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    _, height, width = image.shape\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image.reshape(height, width), cmap=\"gray\", vmin=0, vmax=255)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if save_fig:\n",
    "        plt.savefig(f\"fig_{title}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_after_conv(size_in: tuple, kernel_size, stride, padding):\n",
    "    h_in, w_in = size_in\n",
    "    h_out = int(\n",
    "        np.floor((h_in + 2 * padding - (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "    )\n",
    "    w_out = int(\n",
    "        np.floor((w_in + 2 * padding - (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "    )\n",
    "    return h_out, w_out\n",
    "\n",
    "\n",
    "def size_after_conv_convtranspose(size_in, kernel_size, stride, padding):\n",
    "    h_in, w_in = size_in\n",
    "    h_out = (h_in - 1) * stride[0] - 2 * padding + (kernel_size[0] - 1) + 1\n",
    "    w_out = (w_in - 1) * stride[1] - 2 * padding + (kernel_size[1] - 1) + 1\n",
    "\n",
    "    return h_out, w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv 1, size = (64, 64)\n",
      "Conv 2, size = (32, 32)\n",
      "Conv 3, size = (16, 16)\n",
      "Conv 4, size = (8, 8)\n",
      "Conv 5, size = (8, 8)\n"
     ]
    }
   ],
   "source": [
    "size = (64, 64)\n",
    "kernel_sizes = [(3, 3) for i in range(5)]\n",
    "strides = [(1, 1), (2, 2), (2, 2), (2, 2), (1, 1)]\n",
    "paddings = [1 for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    size = size_after_conv(size, kernel_sizes[i], strides[i], paddings[i])\n",
    "    print(f\"Conv {i+1}, size = {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_channels = 1\n",
    "in_channels = 32\n",
    "channel_mults = (1, 2, 2, 2, 2)\n",
    "\n",
    "conv_channels = [img_channels, *map(lambda m: in_channels * m, channel_mults)]\n",
    "encoder_channels = list(zip(conv_channels[:-1], conv_channels[1:]))\n",
    "conv_channels = list(\n",
    "    reversed([img_channels, *map(lambda m: in_channels * m, channel_mults)])\n",
    ")\n",
    "decoder_channels = list(zip(conv_channels[:-1], conv_channels[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 32), (32, 64), (64, 64), (64, 64), (64, 64)],\n",
       " [(64, 64), (64, 64), (64, 64), (64, 32), (32, 1)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_channels, decoder_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, 1), (64, 32), (64, 64), (64, 64), (64, 64)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: tuple | int,\n",
    "        latent_size: int,\n",
    "        in_out_channels: list[tuple],\n",
    "        kernels: list[tuple],\n",
    "        strides: list[tuple],\n",
    "        pads: list[int] | list[tuple],\n",
    "        flattened_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(image_size, int):\n",
    "            image_size = (image_size, image_size)\n",
    "        self.image_size = image_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.convolution_seria = nn.Sequential()\n",
    "        for in_out, kernel, stride, pad in zip(in_out_channels, kernels, strides, pads):\n",
    "            conv = nn.Conv2d(\n",
    "                in_out[0], in_out[1], stride=stride, kernel_size=kernel, padding=pad\n",
    "            )\n",
    "            image_size = size_after_conv(image_size, kernel, stride, pad)\n",
    "            self.convolution_seria.append(conv)\n",
    "            self.convolution_seria.append(nn.LeakyReLU(0.01))\n",
    "        self.convolution_seria.pop(-1)  # delete last activation function\n",
    "        self.convolution_seria.append(nn.Flatten())\n",
    "\n",
    "        self.z_mean = torch.nn.Linear(flattened_size, self.latent_size)\n",
    "        self.z_log_var = torch.nn.Linear(flattened_size, self.latent_size)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: tuple | int,\n",
    "        latent_size: int,\n",
    "        in_out_channels: list[tuple],\n",
    "        kernels: list[tuple],\n",
    "        strides: list[tuple],\n",
    "        pads: list[int] | list[tuple],\n",
    "        flattened_size: int,\n",
    "        unflattened_size: tuple,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(image_size, int):\n",
    "            image_size = (image_size, image_size)\n",
    "        self.image_size = image_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.convolution_transpose_seria = nn.Sequential()\n",
    "        self.convolution_transpose_seria.append(\n",
    "            torch.nn.Linear(self.latent_size, flattened_size)\n",
    "        )\n",
    "        self.convolution_transpose_seria.append(nn.Unflatten(1, unflattened_size))\n",
    "\n",
    "        size = unflattened_size[1:]\n",
    "        for in_out, kernel, stride, pad in zip(in_out_channels, kernels, strides, pads):\n",
    "            conv_transpose = nn.ConvTranspose2d(\n",
    "                in_out[0], in_out[1], stride=stride, kernel_size=kernel, padding=pad\n",
    "            )\n",
    "            size = size_after_conv_convtranspose(size, kernel, stride, pad)\n",
    "            self.convolution_transpose_seria.append(conv_transpose)\n",
    "            self.convolution_transpose_seria.append(nn.LeakyReLU(0.01))\n",
    "        self.convolution_transpose_seria.pop(-1)\n",
    "        self.convolution_transpose_seria.append(Trim(self.image_size))\n",
    "        self.convolution_transpose_seria.append(nn.Sigmoid())\n",
    "\n",
    "        print(size)\n",
    "\n",
    "        # self.convolution_transpose_seria = nn.Sequential(\n",
    "        #     torch.nn.Linear(self.latent_size, 3136),\n",
    "        #     nn.Unflatten(1, (64, 7, 7)),\n",
    "        #     nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "        #     nn.LeakyReLU(0.01),\n",
    "        #     nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n",
    "        #     nn.LeakyReLU(0.01),\n",
    "        #     nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=0),\n",
    "        #     nn.LeakyReLU(0.01),\n",
    "        #     nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),\n",
    "        #     nn.LeakyReLU(0.01),\n",
    "        #     nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0),\n",
    "        #     Trim(self.image_size),  # 1x57x57 -> 1x56x56\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "\n",
    "\n",
    "class Trim(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.size = image_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, : self.size, : self.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: tuple | int,\n",
    "        latent_size: int,\n",
    "        encoder_in_out_channels: list[tuple],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder_in_out = encoder_in_out_channels\n",
    "        self.decoder_in_out = self.get_decoder_in_out_channels(encoder_in_out_channels)\n",
    "        self.image_size_after_conv = self.size_after_conv(image_size, )\n",
    "\n",
    "    def create_encoder(self):\n",
    "        pass\n",
    "\n",
    "    def create_decoder(self):\n",
    "        pass\n",
    "\n",
    "    def get_decoder_in_out_channels(self, encoder_in_out):\n",
    "        decoder_in_out = list(map(lambda x: tuple(reversed(x)), encoder_in_out))\n",
    "        decoder_in_out = list(reversed(decoder_in_out))\n",
    "        return decoder_in_out\n",
    "\n",
    "    def size_after_conv(self, size_in: tuple, kernel_size, stride, padding):\n",
    "        h_in, w_in = size_in\n",
    "        h_out = int(\n",
    "            np.floor((h_in + 2 * padding - (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        )\n",
    "        w_out = int(\n",
    "            np.floor((w_in + 2 * padding - (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        )\n",
    "        return h_out, w_out\n",
    "\n",
    "    def size_after_conv_convtranspose(self, size_in, kernel_size, stride, padding):\n",
    "        h_in, w_in = size_in\n",
    "        h_out = (h_in - 1) * stride[0] - 2 * padding + (kernel_size[0] - 1) + 1\n",
    "        w_out = (w_in - 1) * stride[1] - 2 * padding + (kernel_size[1] - 1) + 1\n",
    "\n",
    "        return h_out, w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 32), (32, 64), (64, 64), (64, 64), (64, 64)],\n",
       " [(64, 64), (64, 64), (64, 64), (64, 32), (32, 1)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(64, 4, encoder_channels)\n",
    "vae.encoder_in_out, vae.decoder_in_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "flatten_size = in_out_channels[-1][-1] * image_size[0] * image_size[1]\n",
    "unflattened_size = (in_out_channels[-1][-1], image_size[0], image_size[1])\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, img_channels, in_channels, channel_mults):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 65)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (convolution_transpose_seria): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=4096, bias=True)\n",
       "    (1): Unflatten(dim=1, unflattened_size=(64, 8, 8))\n",
       "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "    (10): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (11): Trim()\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = [(3, 3) for i in range(5)]\n",
    "strides = [(1, 1), (2, 2), (2, 2), (2, 2), (1, 1)]\n",
    "paddings = [1 for i in range(5)]\n",
    "\n",
    "encoder = Encoder(64, 4, encoder_channels, kernels, strides, paddings, 4096)\n",
    "decoder = Decoder(\n",
    "    (64, 64), 4, decoder_channels, kernels, strides, [1, 1, 0, 0, 0], 4096, (64, 8, 8)\n",
    ")\n",
    "decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
